{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: left;\">\n",
    "<table style=\"width:100%; background-color:transparent;\">\n",
    "  <tr style=\"background-color:transparent;\">\n",
    "    <td style=\"background-color:transparent;\"><img src=\"https://baobablab.github.io/bhb/images/collaborators/upsaclay.png\" width=\"40%\"></td>\n",
    "    <td style=\"background-color:transparent;\"><img src=\"https://baobablab.github.io/bhb/images/collaborators/cea.jpg\" width=\"30%\"></td>\n",
    "  </tr>\n",
    "</table> \n",
    "</div>\n",
    "\n",
    "<center><h1>OpenBHB challenge: predicting brain age with site-effect removal</h1></center>\n",
    "\n",
    "<center><h3>A data challenge on Healthy Controls</h3></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Content\n",
    "\n",
    "1. [Prerequisites](#Software-prerequisites)\n",
    "2. [Introduction about the competition](#Introduction:-what-is-this-challenge-about)\n",
    "3. [The data](#The-data)\n",
    "4. [Evaluation metrics](#Evaluation-metrics)\n",
    "5. [Submission](#Submitting-to-the-online-challenge:-ramp.studio)\n",
    "6. [Where to start](#Where-to-start:-a-simple-MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To download and run this notebook**: download the [full starting kit](https://github.com/ramp-kits/brain_age_with_site_removal), with all the necessary files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software prerequisites\n",
    "\n",
    "This starting kit requires Python 3 and the following dependencies:\n",
    "\n",
    "* `numpy`\n",
    "* `scipy`\n",
    "* `nibabel`\n",
    "* `pandas`\n",
    "* `scikit-learn`\n",
    "* `nilearn`\n",
    "* `matplolib`\n",
    "* `seaborn`\n",
    "* `jupyter`\n",
    "* `progressbar`\n",
    "* `torch`\n",
    "* `ramp-workflow`\n",
    "\n",
    "You can install the dependencies using `pip` with the following command-line:\n",
    "\n",
    "```\n",
    "pip install -U -r requirements.txt\n",
    "```\n",
    "\n",
    "If you are using `conda`, we provide an `environment.yml` file for similar usage:\n",
    "\n",
    "```\n",
    "conda env create -n ramp-brainage-siterm -f environment.yml\n",
    "```\n",
    "\n",
    "Then, you can activate/desactivate the conda environment using:\n",
    "\n",
    "```\n",
    "conda activate brain_age_siterm\n",
    "conda deactivate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: what is this challenge about\n",
    "\n",
    "Modelling brain development and maturation in the healthy population with Machine Learning (ML) from brain MRI images is a fundamental challenge. Biological processes implied are complex and highly heterogeneous between individuals, comprising both environmental and genetic variability across subjects. As a result, there is a need for large MRI datasets including subjects with a wide range of age. However, these datasets are often multi-sites (i.e images are acquired at different hospitals or acquisition centers across the world) and it induces a strong bias in current MRI data, due to difference between scanners (magnetic field, constructor, gradients, etc.)\n",
    "\n",
    "Consequently, this challenge aims at building i) robust ML models that can accuratly predict chronological age from brain MRI while ii) removing non-biological site information from MRI images. We have designed this challenge in the context of **representation learning** and it promotes the development of new ML and **Deep Learning** algorithms.\n",
    "\n",
    "More specifically, aging is associated with grey matter (GM) atrophy. Each year, an adult lose\n",
    "0.1% of GM. We will try to learn a predictor of the chronological age (true age)\n",
    "using GM derived features on a population of healthy control participants.\n",
    "\n",
    "Such a predictor provides the expected **brain age** of a subject. Deviation from\n",
    "this expected brain age indicates acceleration or slowdown of the aging process\n",
    "which may be associated with a pathological neurobiological process or protective factor of aging.\n",
    "\n",
    "The dataset is composed of images coming from various sites, different MRI scanners and acquired under various settings. In order to predict properly the age of participants, one should deal with the site/scanner effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "We propose to use the [openBHB dataset](https://baobablab.github.io/bhb/dataset). This dataset is composed of 3985 samples in the training set and 666 samples in the test set. A private test set is also used to estimate the final metric of the challenge (the 'challenge' metric). When testing your submission locally (with the 'ramp-test' command, see below), this set is replaced by the test set, which will bias the 'challenge' metric. All other metrics remain valid.\n",
    "\n",
    "### Input data\n",
    "\n",
    "All data have been preprocessed uniformly and checked. The dataset is composed of T1w images derived features composed of Quasi-Raw, CAT12 VBM, and FreeSurfer:\n",
    "\n",
    "- Quasi-Raw: minimally preprocessed data were generated using [ANTS bias field correction](https://manpages.debian.org/testing/ants/N4BiasFieldCorrection.1.en.html), [FSL FLIRT](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FLIRT) affine registration with 9 degrees of freedom (no shearing) to the isotropic 1mm MNI template, and the application of a brain mask to remove non-brain tissues in the final images.\n",
    "\n",
    "- CAT12 VBM: Voxel-Based Morphometry (VBM) was performed with [CAT12](http://www.neuro.uni-jena.de/cat). The analysis stream includes non-linear spatial registration to the isotropic 1.5mm MNI template, Gray Matter (GM), White Matter (WM), and CerebroSpinal Fluid (CSF) tissues segmentation, bias correction of intensity non-uniformities, and segmentations modulation by scaling with the amount of volume changes due to spatial registration. VBM is most often applied to investigate the GM. The sensitivity of VBM in the WM is low, and usually, diffusion-weighted imaging is preferred for that purpose. For this reason, only the modulated GM images are shared. Moreover, CAT12 computes GM volumes averaged on the Neuromorphometrics atlas that includes 284 brain cortical and sub-cortical ROI.\n",
    "\n",
    "- FreeSurfer: Cortical analysis was performed with [FreeSurfer \"recon-all\"](https://surfer.nmr.mgh.harvard.edu). The analysis stream includes intensity normalization, skull stripping, segmentation of GM (pial) and WM, hemispheric-based tessellations, topology corrections and inflation, and registration to the \"fsaverage\" template. From the available morphological measures, the Desikan (68 regions) and Destrieux (148 regions) ROI-based surface area, GM volume, average thickness, thickness standard deviation, integraated rectified mean curvature, integrated rectified gaussian curvature, and intrinsic curvature are computed.\n",
    "\n",
    "The `problem.get_[train|test]_data()` function returns as first element the concatenation of CAT12 VBM (519945 features), Quasi-Raw (1827095 features), CAT12 VBM 284 ROIs (284 features), FreeSurfer Desikan 7 channels | 68 ROIs (476 features), and FreeSurfer 7 channels | 148 ROIs (1036 features) data leading to a (n_subjects x 2348836) array. For ROI-based data, the channel and ROI dimensions has been flattened, and for image-based data a GM (for CAT12 VBM) or brain (for Quasi-Raw) mask has been applied using `nilearn.masking.apply_mask`.\n",
    "\n",
    "A `FeatureExtractor` class is proposed in the `submissions/starting_kit/estimator.py` file that simplify the access of each data bloc. Below you will find some use cases of this functionality:\n",
    "\n",
    "```\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append(os.path.realpath(\"submissions/starting_kit\"))\n",
    "from estimator import FeatureExtractor\n",
    "\n",
    "X_flat = np.zeros((1, 2348836), dtype=np.single)\n",
    "X = FeatureExtractor(dtype=\"vbm\").fit(X_flat)\n",
    "print(X.shape)\n",
    "```\n",
    "\n",
    "### Target\n",
    "\n",
    "The `problem.get_[train|test]_data()` function returns as second element the taget vector composed of\n",
    "`age` and `site` information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation metrics \n",
    "\n",
    "In the context of representation learning, a simple way to evaluate a trained model is to perform **linear probing**. Specifically, on the RAMP server, the public training set (available for this challenge) and 2 private test sets (namely *internal* and *external*) are encoded by the trained model, i.e all data are passed through the model and the output vectors are then stacked to form 3 matrices: $X_{train}\\in \\mathbb{R}^{n_{train}\\times p}$, $X_{test}^{int}\\in \\mathbb{R}^{n_{test}\\times p}$, $X_{test}^{ext}\\in \\mathbb{R}^{n_{test}'\\times p}$. Here $p<10^4$ is the model's output dimension (a.k.a latent space dimension)  and $n_{train}=3985, n_{test}=666, n_{test}'=720$. Both training and *internal* test are stratified on both age, sex and **site** (i.e they contain the same sites with similar age and sex distributions). However, the *external* test contains **sites never seen** in training. \n",
    "\n",
    "*Note:* we usually call $X_{train}, X_{test}^{int}, X_{test}^{ext}$ the model **representation**\n",
    "\n",
    "After encoding, a simple linear Ridge regression is trained on $X_{train}$ and tested on $X_{test}^{int}$ and $X_{test}^{int}$ to predict age. The final Mean Absolute Error (MAE, the lower the better) is computed as the reference metric for age prediction and 2 final scores are computed: MAE(internal), computed on $X_{test}^{int}$, and MAE(external), computed on $X_{test}^{ext}$. As for site prediction, a linear logistic regression is also trained on $X_{train}$ and tested on $X_{test}^{int}$, $X_{test}^{ext}$. The final Balanced Accuracy (BAcc) is computed and serves as reference score for site prediction. Bacc should be **equal to random chance** in this challenge, that is $1/n_{sites}=1/64\\approx1.56\\%$ because we want the representation $X_{train}$\n",
    "to *remove* site information from MRI images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submitting to the online challenge: [ramp.studio](http://ramp.studio)\n",
    "\n",
    "### Online submission\n",
    "Once you found a good model, you can submit them to [ramp.studio](http://www.ramp.studio) to enter the online challenge. First, if it is your first time using the RAMP platform, [sign up](http://www.ramp.studio/sign_up), otherwise [log in](http://www.ramp.studio/login). Then sign up to the event [brain_age_with_site_removal](http://www.ramp.studio/events/brain_age_with_site_removal). Sign up for the event. Both signups are controled by RAMP administrators, so there **can be a delay between asking for signup and being able to submit**.\n",
    "\n",
    "Once your signup request is accepted, you can go to your [sandbox](http://www.ramp.studio/events/brain_age_with_site_removal/sandbox) and copy-paste or upload. You can also create a new starting-kit in the `submissions` folder containing these three files `estimator.py`, `weights.pth` and `metadata.pkl` and upload those file directly. You can check the starting-kit ([`estimator.py`](https://github.com/ramp-kits/brain_age_with_site_removal/submissions/starting_kit/estimator.py)) for an example. The submission is only tested on our backend in the similar way as `ramp_test_submission` does it locally. That is the reason why the weights of the trained model needs to be uploaded. Moreover extra info/data can be provided in the metadata pickle file. It may enable to restore the scaler used during the training. While your submission is waiting in the queue and being trained, you can find it in the \"New submissions (pending training)\" table in [my submissions](http://www.ramp.studio/events/brain_age_with_site_removal/my_submissions). Once it is trained, you get a mail, and your submission shows up on the [public leaderboard](http://www.ramp.studio/events/brain_age_with_site_removal/leaderboard). \n",
    "If there is an error (despite having tested your submission locally with `ramp_test_submission`), it will show up in the \"Failed submissions\" table in [my submissions](http://www.ramp.studio/events/brain_age_with_site_removal/my_submissions). You can click on the error to see part of the trace.\n",
    "\n",
    "The data set we use at the backend is usually different from what you find in the starting kit, so the score may be different.\n",
    "\n",
    "The official score in this RAMP (the first score column after \"historical contributivity\" on the [leaderboard](http://www.ramp.studio/events/brain_age_with_site_removal/leaderboard)) is the composite `challenge` metric.\n",
    "\n",
    "### Local submission\n",
    "\n",
    "\n",
    "New submissions need to be located in the `submissions` folder. For instance to create a `linear_regression_rois` submission, start by copying the `starting_kit`:\n",
    "\n",
    "```\n",
    "cp -r submissions/strating_kit submissions/linear_regression_rois`.\n",
    "```\n",
    " \n",
    "Tune the estimator in the `submissions/linear_regression_rois/estimator.py` file. This file must contain a function `get_estimator()` that returns a scikit-learn Pipeline. Then, test your submission locally:\n",
    "\n",
    "```\n",
    "ramp-test --submission linear_regression_rois\n",
    "```\n",
    "\n",
    "Note that the weights of the model are expected in a file called `weights.pth` located in your submission folder `submissions/linear_regression_rois` as well a some extra info/data in a file called `metadata.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to start: a simple MLP\n",
    "\n",
    "We start by downloading the data from the internet using the following command-line:\n",
    "\n",
    "```\n",
    "python download_data.py\n",
    "```\n",
    "\n",
    "The train/test data will be available in the `data` directory. This download step can be long. We also provide a way to create a small random dataset enabling the `--test` option of the script:\n",
    "\n",
    "```\n",
    "python download_data.py --test\n",
    "```\n",
    "\n",
    "In the rest of this notebook we will use this random dataset. We will train a MLP on the average cortical thickness (channel 2) defined from the Desikan parcellation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.realpath(\"submissions/starting_kit\"))\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from joblib.externals.loky.backend.context import get_context\n",
    "from problem import get_train_data, get_test_data, get_cv\n",
    "from estimator import FeatureExtractor, BHBDataset\n",
    "\n",
    "# Need to specify where to find the brain and GM masks using env variables.\n",
    "os.environ[\"VBM_MASK\"] = os.path.join(\n",
    "    os.path.dirname(os.path.dirname(__file__)),\n",
    "    \"cat12vbm_space-MNI152_desc-gm_TPM.nii.gz\")\n",
    "os.environ[\"QUASIRAW_MASK\"] =os.path.join(\n",
    "    os.path.dirname(os.path.dirname(__file__)),\n",
    "    \"quasiraw_space-MNI152_desc-brain_T1w.nii.gz\")\n",
    "\n",
    "# TODO: we selected only few (20) subjects for deployment speedup.\n",
    "train_dataset = BHBDataset(data_loader=get_train_data)\n",
    "X_train, y_train = train_dataset.get_data()\n",
    "X_train = X_train[:20]\n",
    "y_train = y_train[:20]\n",
    "X_select_train = FeatureExtractor(dtype=\"desikan_roi\").transform(X_train)\n",
    "print(X_train.shape, X_select_train.shape)\n",
    "df_labels_train = pd.DataFrame(data=y_train, columns=[\"age\", \"site\"])\n",
    "print(df_labels_train.head())\n",
    "X_train = X_select_train[:, 2]\n",
    "y_train = y_train[:, 0]\n",
    "\n",
    "# TODO: we selected only few (10) subjects for deployment speedup.\n",
    "test_dataset = BHBDataset(data_loader=get_test_data)\n",
    "X_test, y_test = test_dataset.get_data(dtype=\"internal\")\n",
    "X_test = X_test[:10]\n",
    "y_test = y_test[:10]\n",
    "X_select_test = FeatureExtractor(dtype=\"desikan_roi\").transform(X_test)\n",
    "print(X_test.shape, X_select_test.shape)\n",
    "df_labels_test = pd.DataFrame(data=y_test, columns=[\"age\", \"site\"])\n",
    "print(df_labels_test.head())\n",
    "X_test = X_select_test[:, 2]\n",
    "y_test = y_test[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define here some utility functions to compute scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_train_test_scores(rmse_cv_test, rmse_cv_train, r2_cv_test, r2_cv_train,\n",
    "                         y_train, y_pred_train, y_test, y_pred_test):\n",
    "    \"\"\" Compute CV score, train and test score from a cv grid search model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rmse_cv_test : array\n",
    "        Test rmse across CV folds.\n",
    "    rmse_cv_train : array\n",
    "        Train rmse across CV folds.\n",
    "    r2_cv_test : array\n",
    "        Test R2 across CV folds.\n",
    "    r2_cv_train : array\n",
    "        Train R2 across CV folds.\n",
    "    y_train : array\n",
    "        True train values.\n",
    "    y_pred_train : array\n",
    "        Predicted train values.\n",
    "    y_test : array\n",
    "        True test values.\n",
    "    y_pred_test : array\n",
    "        Predicted test values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    info : TYPE\n",
    "        DataFrame(r2_cv, r2_train, mae_train, mse_train).\n",
    "    \"\"\"\n",
    "    # CV scores\n",
    "    rmse_cv_test_mean, rmse_cv_test_sd = (\n",
    "        np.mean(rmse_cv_test), np.std(rmse_cv_test))\n",
    "    rmse_cv_train_mean, rmse_cv_train_sd = (\n",
    "        np.mean(rmse_cv_train), np.std(rmse_cv_train))\n",
    "\n",
    "    r2_cv_test_mean, r2_cv_test_sd = (\n",
    "        np.mean(r2_cv_test), np.std(r2_cv_test))\n",
    "    r2_cv_train_mean, r2_cv_train_sd = (\n",
    "        np.mean(r2_cv_train), np.std(r2_cv_train))\n",
    "\n",
    "    # Test scores\n",
    "    rmse_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))\n",
    "    r2_test = metrics.r2_score(y_test, y_pred_test)\n",
    "\n",
    "    # Train scores\n",
    "    rmse_train = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train))\n",
    "    r2_train = metrics.r2_score(y_train, y_pred_train)\n",
    "\n",
    "    scores = pd.DataFrame(\n",
    "        [[rmse_cv_test_mean, rmse_cv_test_sd, rmse_cv_train_mean,\n",
    "          rmse_cv_train_sd, r2_cv_test_mean, rmse_cv_test_sd, r2_cv_train_mean,\n",
    "          r2_cv_train_sd, rmse_test, r2_test, rmse_train, r2_train]],\n",
    "        columns=(\"rmse_cv_test_mean\", \"rmse_cv_test_sd\", \"rmse_cv_train_mean\",\n",
    "                 \"rmse_cv_train_sd\", \"r2_cv_test_mean\", \"rmse_cv_test_sd\",\n",
    "                 \"r2_cv_train_mean\", \"r2_cv_train_sd\", \"rmse_test\", \"r2_test\",\n",
    "                 \"rmse_train\", \"r2_train\"))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's design a simple MLP age predictor. The framework is evaluated with a cross-validation approach. The metrics used are the root-mean-square error (RMSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\" Define a simple one hidden layer MLP.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features):\n",
    "        super(MLP, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(in_features, 120),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(120, 84),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(84, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\" A torch dataset for regression.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y=None):\n",
    "        \"\"\" Init class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like (n_samples, n_features)\n",
    "            training data.\n",
    "        y: array-like (n_samples, ), default None\n",
    "            target values.\n",
    "        \"\"\"\n",
    "        self.X = torch.from_numpy(X)\n",
    "        if y is not None:\n",
    "            self.y = torch.from_numpy(y)\n",
    "        else:\n",
    "            self.y = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if self.y is not None:\n",
    "            return self.X[i], self.y[i]\n",
    "        else:\n",
    "            return self.X[i]\n",
    "\n",
    "\n",
    "class RegressionModel(object):\n",
    "    \"\"\" Base class for Regression models.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, batch_size=5, n_epochs=3, print_freq=1):\n",
    "        \"\"\" Init class.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model: nn.Module\n",
    "            the input model.\n",
    "        batch_size:int, default 10\n",
    "            the mini_batch size.\n",
    "        n_epochs: int, default 5\n",
    "            the number of epochs.\n",
    "        print_freq: int, default 100\n",
    "            the print frequency.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epochs = n_epochs\n",
    "        self.print_freq = print_freq\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like (n_samples, n_features)\n",
    "            training data.\n",
    "        y: array-like (n_samples, )\n",
    "            target values.\n",
    "        fold: int\n",
    "            the fold index.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        self.reset_weights()\n",
    "        print(\"-- training model...\")\n",
    "        dataset = Dataset(X, y)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=True,\n",
    "            num_workers=1, multiprocessing_context=get_context(\"loky\"))\n",
    "        loss_function = nn.L1Loss()\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        start_time = time.time()\n",
    "        current_loss = 0.\n",
    "        for epoch in range(self.n_epochs):\n",
    "            for step, data in enumerate(loader, start=epoch * len(loader)):\n",
    "                inputs, targets = data\n",
    "                inputs, targets = inputs.float(), targets.float()\n",
    "                targets = targets.reshape((targets.shape[0], 1))\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = loss_function(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                current_loss += loss.item()\n",
    "                if step % self.print_freq == 0:\n",
    "                    stats = dict(epoch=epoch, step=step,\n",
    "                                 lr=optimizer.param_groups[0][\"lr\"],\n",
    "                                 loss=loss.item(),\n",
    "                                 time=int(time.time() - start_time))\n",
    "                    print(json.dumps(stats))\n",
    "        current_loss /= (len(loader) * self.n_epochs)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict using the input model.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: array-like (n_samples, n_features)\n",
    "            samples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        C: array (n_samples, )\n",
    "            returns predicted values.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        dataset = Dataset(X)\n",
    "        testloader = torch.utils.data.DataLoader(\n",
    "            dataset, batch_size=self.batch_size, shuffle=False, num_workers=1,\n",
    "            multiprocessing_context=get_context(\"loky\"))\n",
    "        with torch.no_grad():\n",
    "            C = []\n",
    "            for i, inputs in enumerate(testloader):\n",
    "                inputs = inputs.float() \n",
    "                C.append(self.model(inputs))\n",
    "            C = torch.cat(C, dim=0)\n",
    "        return C.numpy().squeeze()\n",
    "\n",
    "    def reset_weights(self):\n",
    "        \"\"\" Reset all the weights of the model.\n",
    "        \"\"\"\n",
    "        def weight_reset(m):\n",
    "            if hasattr(m, \"reset_parameters\"):\n",
    "                m.reset_parameters()\n",
    "        self.model.apply(weight_reset)\n",
    "\n",
    "\n",
    "# CV\n",
    "cv = get_cv(X_train, y_train)\n",
    "mlp = MLP(68)\n",
    "estimator = make_pipeline(StandardScaler(), RegressionModel(mlp))\n",
    "cv_results = cross_validate(\n",
    "    estimator, X_train, y_train, scoring=[\"neg_root_mean_squared_error\", \"r2\"],\n",
    "    cv=cv, verbose=1, return_train_score=True, n_jobs=5)\n",
    "\n",
    "# Refit on all train\n",
    "estimator.fit(X_train, y_train)\n",
    "\n",
    "# Apply on test\n",
    "y_pred_train = estimator.predict(X_train)\n",
    "y_pred_test = estimator.predict(X_test)\n",
    "\n",
    "scores = cv_train_test_scores(\n",
    "    rmse_cv_test=-cv_results[\"test_neg_root_mean_squared_error\"],\n",
    "    rmse_cv_train=-cv_results[\"train_neg_root_mean_squared_error\"],\n",
    "    r2_cv_test=cv_results[\"test_r2\"],\n",
    "    r2_cv_train=cv_results[\"train_r2\"],\n",
    "    y_train=y_train, y_pred_train=y_pred_train, y_test=y_test,\n",
    "    y_pred_test=y_pred_test).T.round(3)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important notes**\n",
    "\n",
    "- We expect from the submitted models to **ouput a vector** of representation for each input data. The output vector is a representation of input data that should fully preserve chronological age information AND remove site information. The vector size should be reasonable and **must** not exceed $10^4$. An error will be raised if this condition is not met.\n",
    "- The dataset is quite large, and memory mapping is used to load the data. In order to minimize the memory footprint (and to avoid errors during the submission on RAMP), we highly recommand to apply the features extraction, data preprocessing and data scaling steps as transforms. Thus all steps will be applied on each mini-batch. An working example can be found on the [`estimator.py`](https://github.com/ramp-kits/brain_age_with_site_removal/submissions/starting_kit/estimator.py) file of the starting kit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
